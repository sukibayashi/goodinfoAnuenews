{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3615\n",
      "已完成1301次\n",
      "3623\n",
      "已完成1302次\n",
      "3630\n",
      "已完成1303次\n",
      "3666\n",
      "已完成1304次\n",
      "3691\n",
      "已完成1305次\n",
      "3713\n",
      "已完成1306次\n",
      "4729\n",
      "已完成1307次\n",
      "4933\n",
      "已完成1308次\n",
      "4972\n",
      "已完成1309次\n",
      "4995\n",
      "已完成1310次\n",
      "5220\n",
      "已完成1311次\n",
      "5230\n",
      "已完成1312次\n",
      "5245\n",
      "已完成1313次\n",
      "5251\n",
      "已完成1314次\n",
      "5281\n",
      "已完成1315次\n",
      "5315\n",
      "已完成1316次\n",
      "5371\n",
      "已完成1317次\n",
      "5392\n",
      "已完成1318次\n",
      "5432\n",
      "已完成1319次\n",
      "5443\n",
      "已完成1320次\n",
      "6125\n",
      "已完成1321次\n",
      "6167\n",
      "已完成1322次\n",
      "6222\n",
      "已完成1323次\n",
      "6234\n",
      "已完成1324次\n",
      "6244\n",
      "已完成1325次\n",
      "6246\n",
      "已完成1326次\n",
      "6419\n",
      "已完成1327次\n",
      "6556\n",
      "已完成1328次\n",
      "6560\n",
      "已完成1329次\n",
      "7402\n",
      "已完成1330次\n",
      "8049\n",
      "已完成1331次\n",
      "8064\n",
      "已完成1332次\n",
      "8069\n",
      "已完成1333次\n",
      "8087\n",
      "已完成1334次\n",
      "8111\n",
      "已完成1335次\n",
      "8240\n",
      "已完成1336次\n",
      "3081\n",
      "已完成1337次\n",
      "3095\n",
      "已完成1338次\n",
      "3152\n",
      "已完成1339次\n",
      "3163\n",
      "已完成1340次\n",
      "3221\n",
      "已完成1341次\n",
      "3234\n",
      "已完成1342次\n",
      "3306\n",
      "已完成1343次\n",
      "3363\n",
      "已完成1344次\n",
      "3466\n",
      "已完成1345次\n",
      "3491\n",
      "已完成1346次\n",
      "3499\n",
      "已完成1347次\n",
      "3558\n",
      "已完成1348次\n",
      "3564\n",
      "已完成1349次\n",
      "3632\n",
      "已完成1350次\n",
      "3664\n",
      "已完成1351次\n",
      "3672\n",
      "已完成1352次\n",
      "3684\n",
      "已完成1353次\n",
      "4903\n",
      "已完成1354次\n",
      "4905\n",
      "已完成1355次\n",
      "4908\n",
      "已完成1356次\n",
      "4909\n",
      "已完成1357次\n",
      "4979\n",
      "已完成1358次\n",
      "5348\n",
      "已完成1359次\n",
      "5353\n",
      "已完成1360次\n",
      "6109\n",
      "已完成1361次\n",
      "6143\n",
      "已完成1362次\n",
      "6163\n",
      "已完成1363次\n",
      "6170\n",
      "已完成1364次\n",
      "6190\n",
      "已完成1365次\n",
      "6218\n",
      "已完成1366次\n",
      "6241\n",
      "已完成1367次\n",
      "6245\n",
      "已完成1368次\n",
      "6263\n",
      "已完成1369次\n",
      "6417\n",
      "已完成1370次\n",
      "6465\n",
      "已完成1371次\n",
      "6470\n",
      "已完成1372次\n",
      "6486\n",
      "已完成1373次\n",
      "6514\n",
      "已完成1374次\n",
      "6530\n",
      "已完成1375次\n",
      "6546\n",
      "已完成1376次\n",
      "6561\n",
      "已完成1377次\n",
      "6588\n",
      "已完成1378次\n",
      "8034\n",
      "已完成1379次\n",
      "8048\n",
      "已完成1380次\n",
      "8059\n",
      "已完成1381次\n",
      "8089\n",
      "已完成1382次\n",
      "8097\n",
      "已完成1383次\n",
      "8171\n",
      "已完成1384次\n",
      "8176\n",
      "已完成1385次\n",
      "1336\n",
      "已完成1386次\n",
      "1595\n",
      "已完成1387次\n",
      "1815\n",
      "已完成1388次\n",
      "3078\n",
      "已完成1389次\n",
      "3089\n",
      "已完成1390次\n",
      "3114\n",
      "已完成1391次\n",
      "3115\n",
      "已完成1392次\n",
      "3191\n",
      "已完成1393次\n",
      "3202\n",
      "已完成1394次\n",
      "3206\n",
      "已完成1395次\n",
      "3207\n",
      "已完成1396次\n",
      "3217\n",
      "已完成1397次\n",
      "3236\n",
      "已完成1398次\n",
      "3276\n",
      "已完成1399次\n",
      "3288\n",
      "已完成1400次\n",
      "爬蟲完畢\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import sqlite3\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# 連接到資料庫A ====================================================================\n",
    "conn = sqlite3.connect('goodinfoRevenue.db')\n",
    "# cursor object\n",
    "cursor = conn.cursor()\n",
    "\n",
    "sql = '''select * from revenue t'''\n",
    "\n",
    "# 連接到資料庫B\n",
    "sqlStock = '''select * from stock t'''\n",
    "dfStock = pd.read_sql(sqlStock,conn)\n",
    "dfStock = pd.DataFrame(dfStock)\n",
    "ids = dfStock['code']\n",
    "# ===================================================================================\n",
    "\n",
    "count=0\n",
    "\n",
    "for id in ids:\n",
    "    print(id)\n",
    "\n",
    "    # 上個月財報日期\n",
    "    thisMonth = pd.Timestamp.today() \n",
    "    LastMonth = thisMonth - pd.DateOffset(months=1) # 這個月日期減上個月日期\n",
    "    LastMonth = LastMonth.strftime(\"%Y/%m\") # 格式化日期\n",
    "\n",
    "    # 先查詢本地資料庫\n",
    "    dfdatabase = pd.read_sql(sql,conn)\n",
    "    dfdatabaseMask = dfdatabase['code'] == id\n",
    "    dfdatabase2 = dfdatabase[dfdatabaseMask]\n",
    "    dfdatabase2\n",
    "\n",
    "    # 如果沒有上個月資料則爬蟲更新\n",
    "    if LastMonth not in dfdatabase2['date'].values:\n",
    "        \n",
    "        # 爬個股資料\n",
    "        url = f'https://goodinfo.tw/tw/ShowSaleMonChart.asp?STOCK_ID={id}'\n",
    "        headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/104.0.5112.102 Safari/537.36 Edg/104.0.1293.70'} # goodinfo有擋機器人爬蟲，透過添加headers模仿真實上網的環境就能抓到資料了\n",
    "        res = requests.get(url, headers = headers,timeout=20)\n",
    "        res.encoding = \"utf-8\" # 將編碼設定為【utf-8】，中文字就能顯示出來了\n",
    "        # res.text\n",
    "\n",
    "        # BeautifSoup是一個用來解析HTML結構的Python套件，將取回的網頁HTML結構透過提供的方法解析。解析器（html.parser,html5lib,lxml），官方文件lxml為最快\n",
    "        soup = BeautifulSoup(res.text,\"lxml\") \n",
    "        # select_one：搜索類名、標籤名、id名等，因為我們搜索的是id，在html語言中要加【#】才能搜索到\n",
    "        data = soup.select_one(\"#divSaleMonChartDetail\")\n",
    "        # data\n",
    "\n",
    "\n",
    "        # 【重整表格】 ===============================================================================================\n",
    "        # prettify()：函數將我們的data物件美化作用\n",
    "        dfs = pd.read_html(data.prettify())\n",
    "        df = dfs[1]\n",
    "        # 網頁的表格是由四格組成，但Python中無法合併單元格一起顯示，所以被合併的表格就會拆分成一格一格顯示\n",
    "        # 使用columns.get_level_values來取得的最後一行的欄位名\n",
    "        df.columns = df.columns.get_level_values(2)\n",
    "        # 刪除所有多於的標題欄\n",
    "        df2 = df[df[\"月別\"]==\"月別\"].index\n",
    "        df2 = df.drop(df2)\n",
    "        # 重整標題\n",
    "        # df2.columns = ['月別','開盤','收盤','最高','最低','漲跌(元)','漲跌(%)','月營收(億)','月月增(%)','月年增','累月營收(億)','累月年增','營收(億)','月增(%)','年增(%)','累計營收(億)','累計年增(%)']\n",
    "        df2.columns = ['date','open','close','high','low','updownYuan','updown','月營收(億)','月月增(%)','月年增','累月營收(億)','累月年增','revenue','mon','yoy','revenueSum','yoySum']\n",
    "\n",
    "        # 刪除營業收入\n",
    "        df3 = df2.copy()\n",
    "        df3.drop(columns=['月營收(億)','月月增(%)','月年增','累月營收(億)','累月年增'],inplace=True)\n",
    "        # 使用pandas的insert方法，第一个参数指定插入列的位置，第二个参数指定插入列的列名，第三个参数指定插入列的数据\n",
    "        df3.insert(0,'code',id)\n",
    "        # =============================================================================================================\n",
    "\n",
    "\n",
    "        # 如果爬蟲沒有新資料則跳過\n",
    "        if LastMonth not in df3['date'].values:\n",
    "            pass\n",
    "        # 如果爬蟲有新資料則更新\n",
    "        else:\n",
    "            df3mask = df3['date'] == LastMonth\n",
    "            df4 = df3[df3mask]\n",
    "            for index, row in df4.iterrows():\n",
    "                try:\n",
    "                    cursor.execute(\n",
    "                    \"\"\"INSERT OR IGNORE INTO revenue \n",
    "                        (code,date,open,close,high,low,updownYuan,updown,revenue,mon,yoy,revenueSum,yoySum)\n",
    "                        values(?,?,?,?,?,?,?,?,?,?,?,?,?)\"\"\",\n",
    "                        (row['code'],\n",
    "                        row['date'],\n",
    "                        row['open'],\n",
    "                        row['close'],\n",
    "                        row['high'],\n",
    "                        row['low'],\n",
    "                        row['updownYuan'],\n",
    "                        row['updown'],\n",
    "                        row['revenue'],\n",
    "                        row['mon'],\n",
    "                        row['yoy'],\n",
    "                        row['revenueSum'],\n",
    "                        row['yoySum'])\n",
    "                        )\n",
    "                    conn.commit()\n",
    "                except:\n",
    "                    pass\n",
    "        count+=1\n",
    "        print(f'已完成{count}次')\n",
    "        time.sleep(75)\n",
    "    # 如果有的話則跳過\n",
    "    else:\n",
    "        count+=1\n",
    "        print(f'已完成{count}次')\n",
    "        pass\n",
    "\n",
    "\n",
    "    \n",
    "# =============================================================================================================\n",
    "# pd.read_sql(sql,conn)\n",
    "print('爬蟲完畢')\n",
    "conn.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit (microsoft store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0d17bd3c188470cc3cbfd93c90c2f8b3f5ab1a762ae997509e3112b6584543eb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
