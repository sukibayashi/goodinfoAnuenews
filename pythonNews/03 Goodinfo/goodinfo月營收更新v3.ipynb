{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "正在执行第0次: 0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "爬蟲完畢\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import sqlite3\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "count = 0\n",
    "\n",
    "def 連接到資料庫(db):\n",
    "    global conn,cursor\n",
    "    # 連接到資料庫A ====================================================================\n",
    "    conn = sqlite3.connect('goodinfoRevenue.db')\n",
    "    # cursor object\n",
    "    cursor = conn.cursor()\n",
    "\n",
    "    sql = '''select * from revenue t'''\n",
    "    dfdatabase = pd.read_sql(sql,conn)\n",
    "\n",
    "    # 連接到資料庫B\n",
    "    sqlStock = '''select * from stock t'''\n",
    "    dfStock = pd.read_sql(sqlStock,conn)\n",
    "    dfStock = pd.DataFrame(dfStock)\n",
    "    ids = dfStock['code'][count:]\n",
    "    # ===================================================================================\n",
    "    return dfdatabase, ids\n",
    "    \n",
    "import random\n",
    "\n",
    "def 導入資料庫(df3, LastMonth):\n",
    "    # 如果爬蟲沒有新資料則跳過\n",
    "    if LastMonth not in df3['date'].values:\n",
    "        # ================================\n",
    "        # print(id)\n",
    "        # ================================\n",
    "        pass\n",
    "    # 如果爬蟲有新資料則更新\n",
    "    else:\n",
    "        df3mask = df3['date'] == LastMonth\n",
    "        df4 = df3[df3mask]\n",
    "        for index, row in df4.iterrows():\n",
    "            try:\n",
    "                cursor.execute(\n",
    "                \"\"\"INSERT OR IGNORE INTO revenue \n",
    "                    (code,date,open,close,high,low,updownYuan,updown,revenue,mon,yoy,revenueSum,yoySum)\n",
    "                    values(?,?,?,?,?,?,?,?,?,?,?,?,?)\"\"\",\n",
    "                    (row['code'],\n",
    "                    row['date'],\n",
    "                    row['open'],\n",
    "                    row['close'],\n",
    "                    row['high'],\n",
    "                    row['low'],\n",
    "                    row['updownYuan'],\n",
    "                    row['updown'],\n",
    "                    row['revenue'],\n",
    "                    row['mon'],\n",
    "                    row['yoy'],\n",
    "                    row['revenueSum'],\n",
    "                    row['yoySum'])\n",
    "                    )\n",
    "                conn.commit()\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "def 爬蟲(count, id, LastMonth):\n",
    "    try:\n",
    "        # 爬個股資料\n",
    "        url = f'https://goodinfo.tw/tw/ShowSaleMonChart.asp?STOCK_ID={id}'\n",
    "        # headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/104.0.5112.102 Safari/537.36 Edg/104.0.1293.70'} # goodinfo有擋機器人爬蟲，透過添加headers模仿真實上網的環境就能抓到資料了\n",
    "        headers = {'User-Agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/112.0.0.0 Safari/537.36 Edg/112.0.1722.39'}\n",
    "        res = requests.get(url, headers = headers, timeout=25)\n",
    "        res.encoding = \"utf-8\" # 將編碼設定為【utf-8】，中文字就能顯示出來了\n",
    "        # res.text\n",
    "\n",
    "        # BeautifSoup是一個用來解析HTML結構的Python套件，將取回的網頁HTML結構透過提供的方法解析。解析器（html.parser,html5lib,lxml），官方文件lxml為最快\n",
    "        soup = BeautifulSoup(res.text,\"lxml\") \n",
    "        # select_one：搜索類名、標籤名、id名等，因為我們搜索的是id，在html語言中要加【#】才能搜索到\n",
    "        data = soup.select_one(\"#divSaleMonChartDetail\")\n",
    "        # data\n",
    "        \n",
    "        # 隨機等待時間\n",
    "        # time.sleep(int(format(random.randint(10,20))))\n",
    "        time.sleep(10)\n",
    "\n",
    "        # 【重整表格】 ===============================================================================================\n",
    "        # prettify()：函數將我們的data物件美化作用\n",
    "        dfs = pd.read_html(data.prettify())\n",
    "        df = dfs[1]\n",
    "        # 網頁的表格是由四格組成，但Python中無法合併單元格一起顯示，所以被合併的表格就會拆分成一格一格顯示\n",
    "        # 使用columns.get_level_values來取得的最後一行的欄位名\n",
    "        df.columns = df.columns.get_level_values(2)\n",
    "        # 刪除所有多於的標題欄\n",
    "        df2 = df[df[\"月別\"]==\"月別\"].index\n",
    "        df2 = df.drop(df2)\n",
    "        # 重整標題\n",
    "        # df2.columns = ['月別','開盤','收盤','最高','最低','漲跌(元)','漲跌(%)','月營收(億)','月月增(%)','月年增','累月營收(億)','累月年增','營收(億)','月增(%)','年增(%)','累計營收(億)','累計年增(%)']\n",
    "        df2.columns = ['date','open','close','high','low','updownYuan','updown','月營收(億)','月月增(%)','月年增','累月營收(億)','累月年增','revenue','mon','yoy','revenueSum','yoySum']\n",
    "\n",
    "        # 刪除營業收入\n",
    "        df3 = df2.copy()\n",
    "        df3.drop(columns=['月營收(億)','月月增(%)','月年增','累月營收(億)','累月年增'],inplace=True)\n",
    "        # 使用pandas的insert方法，第一个参数指定插入列的位置，第二个参数指定插入列的列名，第三个参数指定插入列的数据\n",
    "        df3.insert(0,'code',id)\n",
    "        # =============================================================================================================\n",
    "\n",
    "        導入資料庫(df3, LastMonth)\n",
    "    except:\n",
    "        print(f'{count}没有找到{id}资料')\n",
    "        # time.sleep(int(format(random.randint(15,25))))\n",
    "        time.sleep(10)\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "def 更新月營收(count):\n",
    "    dfdatabase, ids = 連接到資料庫('goodinfoRevenue.db')\n",
    "    \n",
    "    # 上個月財報日期\n",
    "    thisMonth = pd.Timestamp.today() \n",
    "    LastMonth = thisMonth - pd.DateOffset(months=1) # 這個月日期減上個月日期\n",
    "    LastMonth = LastMonth.strftime(\"%Y/%m\") # 格式化日期\n",
    "    # LastMonth = '2023/03'\n",
    "    \n",
    "    # 先篩選掉已經存在於stock的股票\n",
    "    check = dfdatabase[dfdatabase['date'] == LastMonth]['code']\n",
    "    ids = ids[~ids.isin(check)]\n",
    "    \n",
    "    progress_bar = tqdm(ids,desc=\"正在执行第{}次\".format(0))\n",
    "    \n",
    "\n",
    "    for id in progress_bar:\n",
    "        \n",
    "        # 先查詢本地資料庫\n",
    "        dfdatabaseMask = dfdatabase['code'] == id\n",
    "        dfdatabase2 = dfdatabase[dfdatabaseMask]\n",
    "        \n",
    "\n",
    "        # 如果沒有上個月資料則爬蟲更新\n",
    "        if LastMonth not in dfdatabase2['date'].values:\n",
    "            \n",
    "            爬蟲(count, id, LastMonth)\n",
    "            \n",
    "            progress_bar.set_description(\"正在执行第{}次\".format(count+1))\n",
    "            progress_bar.update(1)\n",
    "            \n",
    "            count+=1\n",
    "            time.sleep(int(format(random.randint(15,25))))\n",
    "        # 如果有的話則跳過\n",
    "        else:\n",
    "            progress_bar.set_description(\"正在执行第{}次\".format(count+1))\n",
    "            progress_bar.update(1)\n",
    "            \n",
    "            count+=1\n",
    "            pass\n",
    "        \n",
    "    progress_bar.close()\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    更新月營收(count)\n",
    "    \n",
    "    print('爬蟲完畢')\n",
    "    conn.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "40d3a090f54c6569ab1632332b64b2c03c39dcf918b08424e98f38b5ae0af88f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
