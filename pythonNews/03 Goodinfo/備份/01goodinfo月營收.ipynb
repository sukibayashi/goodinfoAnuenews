{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3430\n",
      "已完成1760次\n",
      "爬蟲完畢\n"
     ]
    }
   ],
   "source": [
    "# https://chenchenhouse.com/goodinfo/\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import sqlite3\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "# # Google增強DataFrame的渲染和導出\n",
    "# from google.colab import data_table\n",
    "# data_table.enable_dataframe_formatter()\n",
    "\n",
    "\n",
    "# 連接到資料庫A ====================================================================\n",
    "conn = sqlite3.connect('goodinfoRevenue.db')\n",
    "# cursor object\n",
    "cursor = conn.cursor()\n",
    "\n",
    "sql = '''select * from revenue t'''\n",
    "\n",
    "# 連接到資料庫B\n",
    "sqlStock = '''select * from stock t'''\n",
    "dfStock = pd.read_sql(sqlStock,conn)\n",
    "dfStock = pd.DataFrame(dfStock)\n",
    "ids = dfStock['code'][1759:]\n",
    "# ===================================================================================\n",
    "\n",
    "count=1759\n",
    "\n",
    "for id in ids:\n",
    "  print(id)\n",
    "  # 爬個股資料\n",
    "  url = f'https://goodinfo.tw/tw/ShowSaleMonChart.asp?STOCK_ID={id}'\n",
    "  headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/104.0.5112.102 Safari/537.36 Edg/104.0.1293.70'} # goodinfo有擋機器人爬蟲，透過添加headers模仿真實上網的環境就能抓到資料了\n",
    "  res = requests.get(url, headers = headers,timeout=15)\n",
    "  res.encoding = \"utf-8\" # 將編碼設定為【utf-8】，中文字就能顯示出來了\n",
    "  # res.text\n",
    "\n",
    "  # BeautifSoup是一個用來解析HTML結構的Python套件，將取回的網頁HTML結構透過提供的方法解析。解析器（html.parser,html5lib,lxml），官方文件lxml為最快\n",
    "  soup = BeautifulSoup(res.text,\"lxml\") \n",
    "  # select_one：搜索類名、標籤名、id名等，因為我們搜索的是id，在html語言中要加【#】才能搜索到\n",
    "  data = soup.select_one(\"#divSaleMonChartDetail\")\n",
    "  # data\n",
    "\n",
    "\n",
    "\n",
    "  # 【重整表格】 ===============================================================================================\n",
    "  # prettify()：函數將我們的data物件美化作用\n",
    "  dfs = pd.read_html(data.prettify())\n",
    "  df = dfs[1]\n",
    "  # 網頁的表格是由四格組成，但Python中無法合併單元格一起顯示，所以被合併的表格就會拆分成一格一格顯示\n",
    "  # 使用columns.get_level_values來取得的最後一行的欄位名\n",
    "  df.columns = df.columns.get_level_values(2)\n",
    "  # 刪除所有多於的標題欄\n",
    "  df2 = df[df[\"月別\"]==\"月別\"].index\n",
    "  df2 = df.drop(df2)\n",
    "  # 重整標題\n",
    "  # df2.columns = ['月別','開盤','收盤','最高','最低','漲跌(元)','漲跌(%)','月營收(億)','月月增(%)','月年增','累月營收(億)','累月年增','營收(億)','月增(%)','年增(%)','累計營收(億)','累計年增(%)']\n",
    "  df2.columns = ['date','open','close','high','low','updownYuan','updown','月營收(億)','月月增(%)','月年增','累月營收(億)','累月年增','revenue','mon','yoy','revenueSum','yoySum']\n",
    "\n",
    "  # 刪除營業收入\n",
    "  df3 = df2.copy()\n",
    "  df3.drop(columns=['月營收(億)','月月增(%)','月年增','累月營收(億)','累月年增'],inplace=True)\n",
    "  # 使用pandas的insert方法，第一个参数指定插入列的位置，第二个参数指定插入列的列名，第三个参数指定插入列的数据\n",
    "  df3.insert(0,'code',id)\n",
    "  # =============================================================================================================\n",
    "  # df3\n",
    "\n",
    "\n",
    "\n",
    "  # 【添加進資料庫】 =========================================================================================== \n",
    "  # for index, row in df3.iterrows():\n",
    "  #   print(row['代碼'],row['月別'],row['開盤'],row['收盤'],row['最高'],row['最低'],row['漲跌(元)'],row['漲跌(%)'],row['營收(億)'],row['月增(%)'],row['年增(%)'],row['營收(億)'],row['年增(%)'])\n",
    "  # for index, row in df3.iterrows():\n",
    "  #   print(row['code'],row['date'],row['open'],row['close'],row['high'],row['low'],row['updownYuan'],row['updown'],row['revenue'],row['mon'],row['yoy'],row['revenueSum'],row['yoySum'])\n",
    "\n",
    "  for index, row in df3.iterrows():\n",
    "    try:\n",
    "      cursor.execute(\n",
    "      \"\"\"INSERT OR IGNORE INTO revenue \n",
    "          (code,date,open,close,high,low,updownYuan,updown,revenue,mon,yoy,revenueSum,yoySum)\n",
    "          values(?,?,?,?,?,?,?,?,?,?,?,?,?)\"\"\",\n",
    "          (row['code'],\n",
    "          row['date'],\n",
    "          row['open'],\n",
    "          row['close'],\n",
    "          row['high'],\n",
    "          row['low'],\n",
    "          row['updownYuan'],\n",
    "          row['updown'],\n",
    "          row['revenue'],\n",
    "          row['mon'],\n",
    "          row['yoy'],\n",
    "          row['revenueSum'],\n",
    "          row['yoySum'])\n",
    "          )\n",
    "      conn.commit()\n",
    "    except:\n",
    "      pass\n",
    "\n",
    "  count+=1\n",
    "  print(f'已完成{count}次')\n",
    "  time.sleep(40)\n",
    "# =============================================================================================================\n",
    "# pd.read_sql(sql,conn)\n",
    "print('爬蟲完畢')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit (windows store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0d17bd3c188470cc3cbfd93c90c2f8b3f5ab1a762ae997509e3112b6584543eb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
